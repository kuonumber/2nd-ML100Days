# 2nd-ML100Days

## D2
  * [basic pandas](https://bookdata.readthedocs.io/en/latest/base/01_pandas.html#DataFrame-%E5%85%A5%E9%97%A8)
  
## D3
  * [使用Request抓取資料](https://blog.gtwang.org/programming/python-requests-module-tutorial/) 
  * [字串分割](http://www.runoob.com/python/att-string-split.html)
  * [例外處理:Try-Except](https://pydoing.blogspot.com/2011/01/python-try.html)
  * [隨機產生數值](https://blog.csdn.net/christianashannon/article/details/78867204)
  
## D4
  * [Pandas 數據類型](https://blog.csdn.net/claroja/article/details/72622375)

## D5 
  * [敘述統計與機率分佈 by 吳漢銘老師](http://www.hmwu.idv.tw/web/R_AI_M/AI-M1-hmwu_R_Stat&Prob.pdf)
  * 常見的機率分佈 
    * [Standard Statistical Distributions](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/statistical-distributions])
    * [List of probability distributions](https://en.wikipedia.org/wiki/List_of_probability_distributions)
  * [Descriptive Statistics For pandas Dataframe](https://chrisalbon.com/python/data_wrangling/pandas_dataframe_descriptive_stats/)
  * [pandas 中的繪圖函數](https://amaozhao.gitbooks.io/pandas-notebook/content/pandas%E4%B8%AD%E7%9A%84%E7%BB%98%E5%9B%BE%E5%87%BD%E6%95%B0.html)

## D6
 * [Ways to Detect and Remove the Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)
 * [How to Use Statistics to Identify Outliers in Data](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)
 * [ECDF in python without step function?](https://stackoverflow.com/questions/14006520/ecdf-in-python-without-step-function)
 * [ECDF wiki](https://zh.wikipedia.org/wiki/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0)

## D7  
 * [Is it a good practice to always scale/normalize data for machine learning?](https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning)

## D8
 * [pandas cheat sheet - DataCamp](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)

## D9
 * [statstutor- 介紹相關係數](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf)
 * [guessthecorrelation](http://guessthecorrelation.com/)
 
## D10
 * [核密度估计 Kernel Density Estimation(KDE)](https://blog.csdn.net/unixtch/article/details/78556499)
 * [核密度估计基础-Part I](https://blog.csdn.net/david830_wu/article/details/66974189)

## D11
 * [Python Graph Gallery (圖表參考)](https://python-graph-gallery.com/)

## D12
* [连续特征的离散化](https://www.zhihu.com/question/31989952)

## D14
* [Multiple Subplots](https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html)
* [Seaborn jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html)

## D17
* [特征工程到底是什么？](https://www.zhihu.com/question/29316149)
* [為什麼特徵工程很重要](https://ithelp.ithome.com.tw/articles/10200041?sc=iThelpR)
* [使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)
![](https://ai100-fileentity.cupoy.com/2nd/homework/D17/1556779129615/large)

## D18
* [Built-in Types Python 官方說明](https://docs.python.org/3/library/stdtypes.html)

## D19
* [數據缺失值處理](https://juejin.im/post/5b5c4e6c6fb9a04f90791e0c)
* [数据标准化/归一化](https://blog.csdn.net/pipisorry/article/details/52247379)
![](https://ai100-fileentity.cupoy.com/2nd/homework/D19/1556780749687/large)

## D20
* [離群值](https://zhuanlan.zhihu.com/p/33468998)

## D21
* [偏度與峰度及其python 實現](https://blog.csdn.net/u013555719/article/details/78530879)

## D22
* [數據預處理：獨熱編碼（One-Hot Encoding）和 LabelEncoder標籤編碼](https://www.twblogs.net/a/5baab6e32b7177781a0e6859/zh-cn/)

## D23
* [平均數編碼](https://zhuanlan.zhihu.com/p/26308272)

## D24
* [Feature hashing](https://blog.csdn.net/laolu1573/article/details/79410187)
* [基于sklearn的文本特征抽取](https://www.jianshu.com/p/063840752151)

## D25
* [python-base-datetime](http://www.wklken.me/posts/2015/03/03/python-base-datetime.html)
* [encoding-cyclical-features-for-deep-learning](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)
* [encoding-cyclical-features-24hour-time](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/)
* [feature-engineering-cyclical-features](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)

## D26
* [简单高效的组合特征自动挖掘框架](https://zhuanlan.zhihu.com/p/42946318)

## D27 
* [利用python数据分析之数据聚合与分组（七）](https://zhuanlan.zhihu.com/p/27590154)

## D28
* [特征选择](https://zhuanlan.zhihu.com/p/32749489)
* [Feature Selection](https://github.com/htygithub/machine-learning-python/blob/master/Feature_Selection/intro.md)

## D29
* [permutation importance](https://www.kaggle.com/dansbecker/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights)
* [机器学习 - 特征选择算法流程、分类、优化与发展综述](https://juejin.im/post/5a1f7903f265da431c70144c)

## D30
* [CTR预估[十一]: Algorithm-GBDT Encoder](https://zhuanlan.zhihu.com/p/31734283)
* [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py)
* [三分鐘了解推薦系統中的分解機方法（Factorization Machine, FM）](https://kknews.cc/zh-tw/other/62k4rml.html)

## D31
* [機器學習的機器是怎麼從資料中「學」到東西的？超簡單機器學習名詞入門篇！](https://kopu.chat/2017/07/28/%E6%A9%9F%E5%99%A8%E6%98%AF%E6%80%8E%E9%BA%BC%E5%BE%9E%E8%B3%87%E6%96%99%E4%B8%AD%E3%80%8C%E5%AD%B8%E3%80%8D%E5%88%B0%E6%9D%B1%E8%A5%BF%E7%9A%84%E5%91%A2/)
* [Stanford 李飛飛教授 TED Talk - 如何教懂電腦看圖像](https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language=zh-tw)
* [ML Lecture 0-1: Introduction of Machine Learning](https://www.youtube.com/watch?v=CXgbekl66jc)

## D32
* [Google AI blog](https://ai.googleblog.com/)
* [Facebook Research blog](https://research.fb.com/blog/)
* [Apple machine learning journal](https://machinelearning.apple.com/)
* [機器之心](https://www.jiqizhixin.com/)
* [雷鋒網](http://www.leiphone.com/category/ai)

## D33
* [學習曲線與 bias/variance trade-off](http://bangqu.com/yjB839.html)

## D34
* [台大電機李宏毅教授講解訊練/驗證/測試集的意義](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&feature=youtu.be&t=1948)

## D35
* [如何辨別機器學習模型的好壞？秒懂Confusion Matrix](https://www.ycc.idv.tw/confusion-matrix.html)
* [ROC curves and Area Under the Curve explained (video)](https://www.dataschool.io/roc-curves-and-auc-explained/)
* [多標籤分類是什麼?](https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/learning-model-multi-lable-classification-74d2799ff395)

## D36
* [机器学习模型评估](https://zhuanlan.zhihu.com/p/30721429)

## D37 
* [逻辑回归常见面试题总结](https://blog.csdn.net/qq_23269761/article/details/81778585c)
* [Andrew Ng - Linear regression](https://zh-tw.coursera.org/lecture/machine-learning/cost-function-rkTp3)

## D38 
* [homemade-machine-learning](https://github.com/trekhleb/homemade-machine-learning)
* [2 WAYS TO IMPLEMENT MULTINOMIAL LOGISTIC REGRESSION IN PYTHON](http://dataaspirant.com/2017/05/15/implement-multinomial-logistic-regression-python/)

## D39 - D40
* [什么是稀疏特征(Sparse Features)?](https://www.zhihu.com/question/31951092)
* [What is Multicollinearity? Extensive video + simulation!](https://www.youtube.com/watch?v=Cba9LJ9lS8s)
* [Regularized Regression | 正規化迴歸](https://www.jamleecute.com/regularized-regression-ridge-lasso-elastic/)
* [讀者提問：多元迴歸分析的變數選擇](https://taweihuang.hpd.io/2016/09/12/%E8%AE%80%E8%80%85%E6%8F%90%E5%95%8F%EF%BC%9A%E5%A4%9A%E5%85%83%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90%E7%9A%84%E8%AE%8A%E6%95%B8%E9%81%B8%E6%93%87/)
* [Linear least squares, Lasso,ridge regression有何本质区别？](https://www.zhihu.com/question/38121173)

## D41
* [資料分析&機器學習 決策樹(Decision Tree)以及隨機森林(Random Forest)介紹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)
* [how-decision-tree-algorithm-works](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)

## D49
* [機器學習技法 Lecture 7: Blending and Bagging](https://www.youtube.com/watch?v=mjUKsp0MvMI&list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&index=27&t=0s)

## D54
* [Lecture 1.3 — Introduction Unsupervised Learning —  Machine Learning | Andrew Ng](https://www.youtube.com/watch?v=jAA2g9ItoAc)
* [機器學習: 非監督式學習 (Machine Learning: Unsupervised Learning)](https://murphymind.blogspot.com/2017/06/machine-learning-unsupervised-learning.html)

## D63
* [3 分鐘搞懂深度學習到底在深什麼](https://panx.asia/archives/53209)
* [人工智慧大歷史](https://medium.com/@suipichen/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E5%A4%A7%E6%AD%B7%E5%8F%B2-ffe46a350543)

## D65
* [深度学习网络调参技巧](https://zhuanlan.zhihu.com/p/24720954)
* [Understanding neural networks with TensorFlow Playground](https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground)

## D66
* [實用的tensorflow安裝建議](https://medium.com/@johnnyliao/%E5%9C%A8win10%E4%B8%8A%E5%AE%89%E8%A3%9Dcuda-toolkit-cudnn-tensorflow-gpu-1-12%E4%BB%A5%E4%B8%8B%E5%8F%8A1-13%E4%BB%A5%E4%B8%8A-%E7%9A%84%E5%AE%89%E8%A3%9D%E7%B6%93%E9%A9%97%E5%88%86%E4%BA%AB-c792953b316f)


## D70
* [機器學習 - 神經網路 (多層感知機 Multilayer perceptron, MLP) 運作方式](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af) 
* [多層感知機](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8)

## D71
* [Reference 激活函數的圖示及其一階導數](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)
* [神經網路常用啟動函數總結](https://zhuanlan.zhihu.com/p/39673127)

## D72
* [Dead ReLU Problem' 产生的原因](https://blog.csdn.net/disiwei1012/article/details/79204243)
* [由 Softmax 想到的（三）—— 从 Sigmoid 到 ReLU，一些常见的激活函数](http://sakigami-yang.me/2017/08/11/thinking-from-softmax-03/)
